{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#78 Usando os serviços de AI do GROQ no Python\n",
    "\"\"\"Chamada ao modelo com stream=True\n",
    "Ao usar stream=True, você está dizendo ao modelo para enviar a resposta em partes, conforme ela é gerada, em vez de esperar a resposta completa.\n",
    "Isso é útil para:\n",
    "    Melhorar a experiência do usuário (resposta começa a aparecer imediatamente)\n",
    "    Reduzir latência percebida\n",
    "    Exibir respostas longas de forma progressiva\n",
    "    flush=True força a impressão imediata no terminal\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "client = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explique a diferença entre llm e langchain. Responda em português\"\n",
    "    }\n",
    "]\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    messages=mensagens,\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    stream=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm (Large Language Model) e LangChain são ambos ferramentas voltadas à inteligência artificial, mas servem a finalidades diferentes.\n",
      "\n",
      "**Llm (Large Language Model):**\n",
      "\n",
      "Um Llm é um modelo de linguagem grande que pode processar grandes quantidades de texto e entender o contexto da linguagem natural. Esses modelos são treinados em grandes conjuntos de dados de texto, o que lhes permite aprender padrões e relações dentro da linguagem. Os Llms são frequentemente usados em aplicativos como:\n",
      "\n",
      "- Tradução automática\n",
      "- Respostas às perguntas feitas a um chatbot\n",
      "- Gerenciamento de texto e sentimento\n",
      "- Composição automática de texto\n",
      "\n",
      "Exemplos de Llm incluem os modelos BERT, RoBERTa e transformer.\n",
      "\n",
      "**LangChain:**\n",
      "\n",
      "O LangChain é uma plataforma de desenvolvimento de software que permite criar modelos de linguagem personalizados, chamados de \"Chains\". Esses Chains podem ser usados para criar aplicativos de inteligência artificial, como conversões humanas, assistentes virtuais, etc. O LangChain fornece uma estrutura para combinar vários modelos de linguagem em série, com a intenção de criar modelos mais poderosos e personalizados.\n",
      "\n",
      "Enquanto os Llm são modelos pré-treinados e podem ser utilizados diretamente em aplicações, o LangChain é uma ferramenta que permite a construção de modelos de linguagem personalizados e integráveis.\n",
      "\n",
      "**Diferença clave:**\n",
      "\n",
      "A principal diferença entre Llm e LangChain é que o Llm é um modelo pré-treinado que pode ser usado diretamente em aplicações, enquanto o LangChain é uma plataforma de desenvolvimento de software que permite a construção de modelos de linguagem personalizados. Em outras palavras, o Llm é uma ferramenta, enquanto o LangChain é uma plataforma que permite a criação de ferramentas personalizadas.\n",
      "\n",
      "Em resumo, o Llm é um modelo de linguagem pré-treinado que pode ser usado em aplicações, enquanto o LangChain é uma plataforma de desenvolvimento de software que permite a construção de modelos de linguagem personalizados para aplicações.None"
     ]
    }
   ],
   "source": [
    "#exibir o resultado.\n",
    "for chunck in stream:\n",
    "    print(chunck.choices[0].delta.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
