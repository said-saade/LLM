import openai
from colorama import Fore, Style, init

client = openai.Client()

# Inicializando o colorama

init(autoreset=True)

## 2) a fun√ß√£o abaixo chama o chat gpt para obter a resposta
def geracao_texto(mensagens):
    resposta = client.chat.completions.create(
        messages=mensagens,
        model="gpt-3.5-turbo-0125",
        temperature=0,
        max_tokens=1000,
        stream=True
    )
    print(f"{Fore.CYAN}Bot:", end="")
    texto_completo = ""
    for resposta_stream in resposta:
        texto = resposta_stream.choices[0].delta.content
        if texto:
            ## 3) Ap√≥s obter a resposta, √© exibido a ultima mensagem retornada pelo chat gpt
            ## que utiliza esse la√ßo for para exibir a mensagem quebrada/formatada por linha
            print(texto, end="")
            texto_completo += texto
    print()
    ## 4) ao termino da exibi√ß√£o toda a mensagem √© salva e agregada na variavel mensagens
    ## e retorna para o la√ßo while la embaixo
    ## Quando √© feita uma nova pergunta no la√ßo while la embaixo, √© enviado todo o historico
    ##para o modelo LLM atrav√©s da variavel mensagens. LLM intepreta todo o historico e retorna
    ##tamb√©m a resposta com a ultima pergunta. Atrav√©s do [0] que selecionamos qual resposta
    ## que vamos retorna. Brinque depois com essa variavel[0] mudando para [1] e coloque tamb√©m
    ## um print seco da variavel mensagem para entender melhor o funcionalmento.
    mensagens.append({"role":"assistant", "content": texto_completo})
    return mensagens

## 1) Essa √© a fun√ß√£o incial inicializados.
## Ao digitarmos a pergunta, a mesma √© adicionada a variavel mensagens, e
## chamada a func√ß√£o geracao texto.

if __name__ == "__main__":
    print(f"{Fore.BLUE}Bem Vindo ao Chatbotü§ñ")
    mensagens = []
    while True:
        in_user = input(f"{Fore.Green}User: {Style.RESET_ALL} ")
        mensagens.append({"role": "user", "content": in_user})
        mensagens = geracao_texto(mensagens)
